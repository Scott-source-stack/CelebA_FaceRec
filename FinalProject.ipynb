{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d125c62b-82eb-44e4-bdc5-3e087c3e84f2",
   "metadata": {},
   "source": [
    "## Part 1:  Preparing the CelebA Dataset for a Known vs. Unknown Face Recognition Task\n",
    "\n",
    "In this part of the project, we will prepare the CelebA face dataset for a classification task in which the model must decide if the face belongs to a known or unknown individual. Given that the dataset contains over 200 thousand faces with each face showing up anywhere from only a few times to 30+, we will be selecting identities of \"known\" individuals based on a list that only contains the IDs of individuals with 30 or more appearances.\n",
    "\n",
    "1. We read from the `identity_CelebA.txt` file to map each image filename to an identity ID. This gives us the necessary labels for determining which images correspond to which person.\n",
    "\n",
    "2. We create a subset of identities where each chosen ID appears at least 30 times within the dataset. The rest of the identities are excluded from the known-class pool.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ceb6943-e465-4655-bba4-3f862058afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "random.seed(2)\n",
    "\n",
    "df = pd.read_csv(\"data/identity_CelebA.txt\", sep = \" \", header = None, names=[\"filename\", \"id\"])\n",
    "counts = df[\"id\"].value_counts()\n",
    "possible_celebs = counts[counts >= 30].index.tolist()\n",
    "# print(possible_celebs)\n",
    "\n",
    "#Uncomment the print statement below to see a list of celebrity IDs that appear 30 or more times within the dataset.\n",
    "# print(\"Possible celebrity IDs: \", possible_celebs) \n",
    "\n",
    "known_celebs = random.sample(possible_celebs, 10)\n",
    "# print(known_celebs)\n",
    "\n",
    "# creating a list of possible celebs with the filename still associated\n",
    "celebs_with_rows = df[df[\"id\"].isin(possible_celebs)]\n",
    "# print(celebs_with_rows)\n",
    "\n",
    "# known celebs with row data\n",
    "known_with_rows = df[df[\"id\"].isin(known_celebs)]\n",
    "# print(known_with_rows)\n",
    "\n",
    "# One-hot encode the id column\n",
    "one_hot_labels = pd.get_dummies(known_with_rows[\"id\"], prefix=\"id\")\n",
    "# print(one_hot_labels)\n",
    "\n",
    "known_with_onehot = pd.concat([known_with_rows, one_hot_labels], axis=1)\n",
    "# print(known_with_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7cbe41-5184-4498-a4b5-88715f9df041",
   "metadata": {},
   "source": [
    "Resnet 50 is a pretrained model that breaks each image down into multileveled feature vectors. I convert the data to tensor so it is compatible with the model, and run the pretrained model on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0a8001-934c-4907-bd06-77803c65bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model, load resnet remove the classification layer\n",
    "model = resnet50(weights=\"DEFAULT\")\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  \n",
    "model.eval()\n",
    "\n",
    "# set all images to the same size, convert to tensor and normalize to clean data for resnet\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# load image, transform. Run through resnet using no_grad to speed up processing\n",
    "def extract_feature(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = transform(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        feat = model(x).squeeze().numpy()  \n",
    "    return feat\n",
    "\n",
    "features = []\n",
    "\n",
    "# get image data from row send through feature extraction, create list of processed data with filename, id, feature vector, and the labels\n",
    "for _, row in known_with_onehot.iterrows():\n",
    "    filename = row[\"filename\"]\n",
    "    img_path = os.path.join(\"data/img_align_celeba\", filename)\n",
    "\n",
    "    feature_vec = extract_feature(img_path)\n",
    "\n",
    "    features.append({\n",
    "        \"filename\": filename,\n",
    "        \"id\": row[\"id\"],\n",
    "        \"feature\": feature_vec,\n",
    "        **{col: row[col] for col in one_hot_labels.columns}\n",
    "    })\n",
    "\n",
    "feature_df = pd.DataFrame(features)\n",
    "# print(feature_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d15dfc03-7144-4048-8f24-9687d82b8149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6444444444444445\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.89      0.73         9\n",
      "           1       0.82      1.00      0.90         9\n",
      "           2       0.50      0.56      0.53         9\n",
      "           3       0.50      0.67      0.57         9\n",
      "           4       0.67      0.44      0.53         9\n",
      "           5       0.44      0.44      0.44         9\n",
      "           6       0.71      0.56      0.62         9\n",
      "           7       1.00      0.44      0.62         9\n",
      "           8       0.67      0.89      0.76         9\n",
      "           9       0.83      0.56      0.67         9\n",
      "\n",
      "    accuracy                           0.64        90\n",
      "   macro avg       0.68      0.64      0.64        90\n",
      "weighted avg       0.68      0.64      0.64        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split processed data by id and features\n",
    "X = np.stack(feature_df[\"feature\"].values)\n",
    "\n",
    "y = feature_df[\"id\"].values\n",
    "\n",
    "# make y's 0-9 while maintaining relationship with filenames\n",
    "y_raw = feature_df[\"id\"].values\n",
    "unique_ids = np.unique(y_raw)\n",
    "id_to_idx = {old:i for i, old in enumerate(unique_ids)}\n",
    "y = np.array([id_to_idx[x] for x in y_raw])\n",
    "\n",
    "\n",
    "#train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# run data through linear SVC\n",
    "clf = SVC(kernel=\"linear\", probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87654d0-26b7-43b9-b025-5d04ae7db598",
   "metadata": {},
   "source": [
    "The SVC classified 83% at 10% test size, 77% of the test samples at 20% test size and .64% at 30%. The aggressive fluctuations suggest the data set is too small to reliably learn the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bb13c8d-6898-49ec-812c-601437bab874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 Accuracy: 0.8444444444444444\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "probs = clf.predict_proba(X_test)\n",
    "# find percent of correct guesses within top 5 attempts\n",
    "def top_k_accuracy(probs, y_true, k=3):\n",
    "    k = min(k, probs.shape[1])\n",
    "    top_k = np.argsort(probs, axis=1)[:, -k:]\n",
    "    return np.mean([\n",
    "        y_true[i] in top_k[i]\n",
    "        for i in range(len(y_true))\n",
    "    ])\n",
    "\n",
    "print(\"Top-5 Accuracy:\", top_k_accuracy(probs, y_test, k=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac54429-135c-47cd-9c89-213e87fac5b3",
   "metadata": {},
   "source": [
    "We can find the correct id within 96% of attempts within the first five tries. Which isn't nearly as impressive as it sounds when there are only ten options. The top 2 attempts is correct over 84% however which is a strong foundation for a model. With more time and resources this model could effectively learn known faces with a high level of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "197952e0-7d40-4e74-a88e-8a21b9878989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 1.6985\n",
      "Epoch 20/100, Loss: 1.1868\n",
      "Epoch 30/100, Loss: 0.8467\n",
      "Epoch 40/100, Loss: 0.6252\n",
      "Epoch 50/100, Loss: 0.4784\n",
      "Epoch 60/100, Loss: 0.3779\n",
      "Epoch 70/100, Loss: 0.3068\n",
      "Epoch 80/100, Loss: 0.2547\n",
      "Epoch 90/100, Loss: 0.2154\n",
      "Epoch 100/100, Loss: 0.1850\n",
      "Perceptron Accuracy: 0.6333333253860474\n"
     ]
    }
   ],
   "source": [
    "# same data prep as before except reformatted for torch\n",
    "y_raw = feature_df[\"id\"].values\n",
    "\n",
    "unique_ids = np.unique(y_raw)\n",
    "id_to_idx = {old:i for i, old in enumerate(unique_ids)}\n",
    "y = np.array([id_to_idx[x] for x in y_raw])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "input_dim = X_train.shape[1] \n",
    "\n",
    "class PerceptronClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# use cross entropy because it heavily punishes incorrect guesses\n",
    "model = PerceptronClassifier(input_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test).argmax(dim=1)\n",
    "    accuracy = (preds == y_test_t).float().mean().item()\n",
    "\n",
    "print(\"Perceptron Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d98b690-3548-40ce-b450-1c6d6c73b208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 0.0537\n",
      "Epoch 20/50, Loss: 0.0293\n",
      "Epoch 30/50, Loss: 0.0172\n",
      "Epoch 40/50, Loss: 0.0111\n",
      "Epoch 50/50, Loss: 0.0075\n",
      "Linear Regression Classifier Accuracy: 0.6499999761581421\n"
     ]
    }
   ],
   "source": [
    "# Create a one-hot encoded tensor for training labels\n",
    "y_train_onehot = torch.zeros(len(y_train), num_classes)\n",
    "y_train_onehot[torch.arange(len(y_train)), y_train] = 1.0\n",
    "\n",
    "\n",
    "# Create a one-hot encoded tensor for test labels\n",
    "y_test_onehot = torch.zeros(len(y_test), num_classes)\n",
    "y_test_onehot[torch.arange(len(y_test)), y_test] = 1.0\n",
    "\n",
    "\n",
    "# linear regression model\n",
    "class LinearRegressionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model_lr = LinearRegressionClassifier(input_dim, num_classes)\n",
    "\n",
    "# MSE used because it work well with one hot encoding\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# use torch's optimization model\n",
    "optimizer = torch.optim.Adam(model_lr.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    # Clear previous gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # input data to Linear regression model\n",
    "    outputs = model_lr(X_train)\n",
    "    loss = criterion(outputs, y_train_onehot)\n",
    "\n",
    "    # Backpropagate gradients and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# no_grad to save processing time\n",
    "with torch.no_grad():\n",
    "    # Get predicted class indices by taking argmax over class dimension\n",
    "    preds = model_lr(X_test).argmax(dim=1)\n",
    "    accuracy = (preds == y_test_t).float().mean().item()\n",
    "\n",
    "print(\"Linear Regression Classifier Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33912f7-4db5-435b-a581-58040d278975",
   "metadata": {},
   "source": [
    "The perceptron and Linear regression model both seem to perform pretty poorly when used for this dataset. They seem to overfit to the data really quickly suggesting there is not enough data for them to evaluate. A "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
