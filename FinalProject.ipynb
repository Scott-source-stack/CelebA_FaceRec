{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d125c62b-82eb-44e4-bdc5-3e087c3e84f2",
   "metadata": {},
   "source": [
    "## Part 1:  Preparing the CelebA Dataset for a Known vs. Unknown Face Recognition Task\n",
    "\n",
    "In this part of the project, we will prepare the CelebA face dataset for a classification task in which the model must decide if the face belongs to a known or unknown individual. Given that the dataset contains over 200 thousand faces with each face showing up anywhere from only a few times to 30+, we will be selecting identities of \"known\" individuals based on a list that only contains the IDs of individuals with 30 or more appearances.\n",
    "\n",
    "1. We read from the `identity_CelebA.txt` file to map each image filename to an identity ID. This gives us the necessary labels for determining which images correspond to which person.\n",
    "\n",
    "2. We create a subset of identities where each chosen ID appears at least 30 times within the dataset. The rest of the identities are excluded from the known-class pool.\n",
    "\n",
    "3. After filtering, we selected 10 identities. Each one has at least 30 images and all of them were resized in order to fit for each model. In the instance of the MLP and CNN, the images were resized to 64x64 pixels resulting in an input dimentionality of 3x64x64. The final dataset was split into training, validation, and test sets for each identity to ensure each class has balanced representation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ceb6943-e465-4655-bba4-3f862058afb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6958, 6695, 7073, 5703, 10001, 5672, 6878, 1886, 1842, 6399]\n",
      "{6958: 0, 6695: 1, 7073: 2, 5703: 3, 10001: 4, 5672: 5, 6878: 6, 1886: 7, 1842: 8, 6399: 9}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(2)\n",
    "\n",
    "df = pd.read_csv(\"data/identity_CelebA.txt\", sep = \" \", header = None, names=[\"filename\", \"id\"])\n",
    "counts = df[\"id\"].value_counts()\n",
    "possible_celebs = counts[counts >= 30].index.tolist()\n",
    "\n",
    "#Uncomment the print statement below to see a list of celebrity IDs that appear 30 or more times within the dataset.\n",
    "#print(\"Possible celebrity IDs: \", possible_celebs) \n",
    "\n",
    "known_celebs = random.sample(possible_celebs, 10)\n",
    "id_to_label = {}\n",
    "for i in range(len(known_celebs)):\n",
    "    celeb_id = known_celebs[i]\n",
    "    id_to_label[celeb_id] = i\n",
    "print(known_celebs)\n",
    "print(id_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1a422-ad34-4a6c-b1a5-c259af0041d6",
   "metadata": {},
   "source": [
    "**Now lets partition the data into three major splits**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b0a8001-934c-4907-bd06-77803c65bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples:  210\n",
      "Validation Samples:  40\n",
      "Testing Known Samples:  50\n",
      "Testing Unknown Samples:  2000\n",
      "Known Celebrities:  10\n"
     ]
    }
   ],
   "source": [
    "def prep_celeba_splits(df, known_celebs, min_images = 30, train_ratio = 0.7, val_ratio = 0.15, seed = 2):\n",
    "    df_known = df[df[\"id\"].isin(known_celebs)]\n",
    "    df_unknown = df[~df[\"id\"].isin(known_celebs)]\n",
    "\n",
    "    train_rows, val_rows, test_known_rows = [], [], []\n",
    "    for celeb in known_celebs:\n",
    "        df_celeb = df_known[df_known[\"id\"] == celeb].sample(frac = 1, random_state = seed)\n",
    "        n = len(df_celeb)\n",
    "\n",
    "        n_train = int(n * train_ratio)\n",
    "        n_val = int(n * val_ratio)\n",
    "\n",
    "        train_rows.append(df_celeb.iloc[:n_train])\n",
    "        val_rows.append(df_celeb.iloc[n_train:n_train + n_val])\n",
    "        test_known_rows.append(df_celeb.iloc[n_train+n_val:])\n",
    "\n",
    "    train_df = pd.concat(train_rows)\n",
    "    val_df = pd.concat(val_rows)\n",
    "    test_known_df = pd.concat(test_known_rows)\n",
    "\n",
    "    test_unknown_df = df_unknown.sample(2000, random_state = seed)\n",
    "\n",
    "    return train_df, val_df, test_known_df, test_unknown_df\n",
    "\n",
    "def test_the_data(train_df, val_df, test_known_df, test_unknown_df, known_celebs):\n",
    "    print(\"Training Samples: \", len(train_df))\n",
    "    print(\"Validation Samples: \", len(val_df))\n",
    "    print(\"Testing Known Samples: \", len(test_known_df))\n",
    "    print(\"Testing Unknown Samples: \", len(test_unknown_df))\n",
    "    print(\"Known Celebrities: \", len(known_celebs))\n",
    "\n",
    "train_df, val_df, test_known_df, test_unknown_df = prep_celeba_splits(df, known_celebs)\n",
    "test_the_data(train_df, val_df, test_known_df, test_unknown_df, known_celebs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135ef4c-65fc-48f0-be40-de836417d818",
   "metadata": {},
   "source": [
    "**Now that we have split the test and done a few small sanity checks, we can move on to actually prepping the data for the models.**\n",
    "\n",
    "Small disclaimer, but I had a lot of difficulty understanding how to prep image data for the CNN and MLP so I had used the data_loader.py file from this GitHub Repo that seemed to have a nice setup:\n",
    "\n",
    "\n",
    "https://github.com/zamaex96/ML-LSTM-CNN-RNN-MLP/blob/main/data_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88859cca-6d5c-497f-988b-c07f6fee1210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  torch.Size([3, 64, 64])\n",
      "Label:  0\n",
      "Batch image shape: torch.Size([32, 3, 64, 64])\n",
      "Batch labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, df, img_dir, id_to_label, transform = None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.id_to_label = id_to_label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row[\"filename\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.id_to_label[row[\"id\"]]\n",
    "        return image, label\n",
    "\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CelebADataset(train_df, img_dir = \"data/img_align_celeba\", id_to_label = id_to_label, transform = basic_transform)\n",
    "val_dataset = CelebADataset(val_df, img_dir = \"data/img_align_celeba\", id_to_label = id_to_label, transform = basic_transform)\n",
    "\n",
    "#Sanity check\n",
    "x, y = train_dataset[0]\n",
    "print(\"Shape: \", x.shape)\n",
    "print(\"Label: \", y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 32)\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"Batch image shape:\", images.shape)\n",
    "print(\"Batch labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d55d6c-40a7-4362-a900-aba41c625533",
   "metadata": {},
   "source": [
    "## Part 2: Training a CNN Baseline and MLP baseline\n",
    "\n",
    "For this part of the project we want to evalute the accuracy of two baseline models on the identity dataset. We will first test with a Convolutional Nueral Network and then move on to the Multilayer Perceptron. They will be trained on the same set of known identities using identical train and validation splits. The idea is that we can highlight the difference in accuracy given that a CNN perserves spatial structure whereas an MLP cannot.\n",
    "\n",
    "Citation: https://github.com/asabenhur/CS345/blob/ef562f15f2bb5a3ee23615b29291f918e3878132/fall24/notebooks/module07_04_cnns.ipynb#L516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b596a2a5-fc63-4634-9d94-954fae89f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebA_CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size = 5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size = 5)\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training = self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "num_classes = len(id_to_label)\n",
    "cnn_model = CelebA_CNN(num_classes)\n",
    "\n",
    "def train_epoch(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            current = (batch_idx + 1) * len(X)\n",
    "            print(f\"loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def validate(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            total_loss += loss_fn(preds, y).item()\n",
    "            correct += (preds.argmax(1) == y).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = correct / size\n",
    "    print (f\"Validation Accuracy: {accuracy*100:.1f}%, Avg loss: {avg_loss:.4f}\")\n",
    "    return avg_loss, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a5295-0ef6-46c7-95b4-91bcc971e2be",
   "metadata": {},
   "source": [
    "**Now that we have everything ready, lets spin it up and see what sort of accuracy we get.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b1e9601-5d1f-48e0-8d68-2e63815cafd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302838  [   32/  210]\n",
      "Validation Accuracy: 15.0%, Avg loss: 2.2981\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.285281  [   32/  210]\n",
      "Validation Accuracy: 20.0%, Avg loss: 2.2501\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.255957  [   32/  210]\n",
      "Validation Accuracy: 22.5%, Avg loss: 1.9147\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.037496  [   32/  210]\n",
      "Validation Accuracy: 32.5%, Avg loss: 1.8390\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.791372  [   32/  210]\n",
      "Validation Accuracy: 42.5%, Avg loss: 1.4766\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.752915  [   32/  210]\n",
      "Validation Accuracy: 37.5%, Avg loss: 1.5907\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.392975  [   32/  210]\n",
      "Validation Accuracy: 52.5%, Avg loss: 1.4350\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.074172  [   32/  210]\n",
      "Validation Accuracy: 57.5%, Avg loss: 1.2787\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.025722  [   32/  210]\n",
      "Validation Accuracy: 52.5%, Avg loss: 1.3834\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.903976  [   32/  210]\n",
      "Validation Accuracy: 60.0%, Avg loss: 1.5228\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.986098  [   32/  210]\n",
      "Validation Accuracy: 57.5%, Avg loss: 1.2586\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.441921  [   32/  210]\n",
      "Validation Accuracy: 57.5%, Avg loss: 1.3638\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.455201  [   32/  210]\n",
      "Validation Accuracy: 75.0%, Avg loss: 1.3657\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.329786  [   32/  210]\n",
      "Validation Accuracy: 70.0%, Avg loss: 1.0642\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.169054  [   32/  210]\n",
      "Validation Accuracy: 65.0%, Avg loss: 1.4791\n",
      "Training Complete :)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn_model = CelebA_CNN(num_classes).to(device)\n",
    "learning_rate = 0.001\n",
    "epochs = 15\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr = learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = train_epoch(train_loader, cnn_model, loss_fn, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss, val_acc = validate(val_loader, cnn_model, loss_fn, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "print(\"Training Complete :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127036d-13ab-4953-801c-21eac4191640",
   "metadata": {},
   "source": [
    "**We can see that at epochs 11-13 the model approaches its best accuracy and then begins to overfit and memorize the training data.**\n",
    "\n",
    "Now lets look at something we expect to do worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "860de56a-a089-4872-9d20-83621b93bc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12288])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "mlp_transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))\n",
    "])\n",
    "\n",
    "mlp_train_dataset = CelebADataset(train_df, img_dir = \"data/img_align_celeba\", id_to_label = id_to_label, transform = mlp_transform)\n",
    "mlp_val_dataset = CelebADataset(val_df, img_dir = \"data/img_align_celeba\", id_to_label = id_to_label, transform = mlp_transform)\n",
    "#mlp_test_dataset = CelebADataset(test_df, img_dir = \"data/img_align_celeba\", id_to_label = id_to_label, transform = mlp_transform)\n",
    "mlp_train_loader = DataLoader(mlp_train_dataset, batch_size = 32, shuffle = True)\n",
    "mlp_val_loader = DataLoader(mlp_val_dataset, batch_size = 32)\n",
    "#mlp_test_loader = DataLoader(mlp_test_dataset, batch_size = 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e964fcb-7744-498d-8c6b-b86ce8499c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.263547  [   32/  210]\n",
      "Validation Accuracy: 20.0%, Avg loss: 3.2991\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 5.728381  [   32/  210]\n",
      "Validation Accuracy: 17.5%, Avg loss: 2.6118\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.380456  [   32/  210]\n",
      "Validation Accuracy: 27.5%, Avg loss: 1.9695\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.423108  [   32/  210]\n",
      "Validation Accuracy: 30.0%, Avg loss: 1.7995\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.839095  [   32/  210]\n",
      "Validation Accuracy: 37.5%, Avg loss: 1.8151\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.782277  [   32/  210]\n",
      "Validation Accuracy: 30.0%, Avg loss: 1.7303\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.604099  [   32/  210]\n",
      "Validation Accuracy: 45.0%, Avg loss: 1.6536\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.642818  [   32/  210]\n",
      "Validation Accuracy: 42.5%, Avg loss: 1.7160\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.561949  [   32/  210]\n",
      "Validation Accuracy: 42.5%, Avg loss: 1.5814\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.670722  [   32/  210]\n",
      "Validation Accuracy: 42.5%, Avg loss: 1.5364\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.593983  [   32/  210]\n",
      "Validation Accuracy: 37.5%, Avg loss: 1.5601\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.637109  [   32/  210]\n",
      "Validation Accuracy: 47.5%, Avg loss: 1.4375\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.241771  [   32/  210]\n",
      "Validation Accuracy: 55.0%, Avg loss: 1.4657\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.225389  [   32/  210]\n",
      "Validation Accuracy: 55.0%, Avg loss: 1.3823\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.486214  [   32/  210]\n",
      "Validation Accuracy: 45.0%, Avg loss: 1.3206\n",
      "Training Complete :)\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training = self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_dim = 3 * 64 * 64\n",
    "mlp_model = MLP(input_dim, num_classes).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr = learning_rate)\n",
    "\n",
    "mlp_train_losses = []\n",
    "mlp_val_losses = []\n",
    "mlp_val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loss = train_epoch(mlp_train_loader, mlp_model, loss_fn, optimizer, device)\n",
    "    mlp_train_losses.append(train_loss)\n",
    "    mlp_val_loss, mlp_val_acc = validate(mlp_val_loader, mlp_model, loss_fn, device)\n",
    "    mlp_val_losses.append(mlp_val_loss)\n",
    "    mlp_val_accuracies.append(mlp_val_acc)\n",
    "\n",
    "print(\"Training Complete :)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6478e76d-b038-4541-ae63-134ca59e8eb7",
   "metadata": {},
   "source": [
    "**We can tell that there is a noticable change in accuracy between the MLP and the CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "07aa39df-f9bd-412d-afbf-e3ffb7686bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model  Best Validation Accuracy %\n",
      "0   CNN                        75.0\n",
      "1   MLP                        55.0\n"
     ]
    }
   ],
   "source": [
    "best_cnn_accuracy = max(val_accuracies)\n",
    "best_mlp_accuracy = max(mlp_val_accuracies)\n",
    "results_df = pd.DataFrame({\n",
    "    \"Model\": [\"CNN\", \"MLP\"],\n",
    "    \"Best Validation Accuracy %\": [best_cnn_accuracy *100, best_mlp_accuracy * 100]\n",
    "})\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08bc66-3ad0-4fd6-89d9-356cbaee1dee",
   "metadata": {},
   "source": [
    "## Part 3: Results and Discussion\n",
    "\n",
    "Across multiple runs, the convolutional neural network consistently outpreformed the multilayer perceptron by around 20% in validation accuracy. This preformance gap highlights the choice in using a model capable of exploiting spacial structure. Whereas the MLP uses flattened pixel vectors and discards spacial relationships between pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ba930-f285-410e-ada0-d3d9c9429494",
   "metadata": {},
   "source": [
    "## Part 4: Extension - Pretrained Feature Extraction with ResNet-50\n",
    "\n",
    "In this extension we aim to explore an alternative approach to face recognition by using a pretrained feature extraction. Instead of training our models on raw images, like in the above experiment, we use a ResNet-50 model to extract feature embeddings for each image. These embeddings are then used with our linear classifiers. This will show how representation quality affects classification preformance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39d3e21c-1a1c-41f8-aa01-89cfb7f867a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of possible celebs with the filename still associated\n",
    "celebs_with_rows = df[df[\"id\"].isin(possible_celebs)]\n",
    "# print(celebs_with_rows)\n",
    "\n",
    "# known celebs with row data\n",
    "known_with_rows = df[df[\"id\"].isin(known_celebs)]\n",
    "# print(known_with_rows)\n",
    "\n",
    "# One-hot encode the id column\n",
    "one_hot_labels = pd.get_dummies(known_with_rows[\"id\"], prefix=\"id\")\n",
    "# print(one_hot_labels)\n",
    "\n",
    "known_with_onehot = pd.concat([known_with_rows, one_hot_labels], axis=1)\n",
    "# print(known_with_onehot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226ffad-bb4b-479a-97a6-d44e347e922c",
   "metadata": {},
   "source": [
    "Resnet 50 is a pretrained model that breaks each image down into multileveled feature vectors. I convert the data to tensor so it is compatible with the model, and run the pretrained model on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f094e650-0eae-4666-8672-91bb17f3999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model, load resnet remove the classification layer\n",
    "model = resnet50(weights=\"DEFAULT\")\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  \n",
    "model.eval()\n",
    "\n",
    "# set all images to the same size, convert to tensor and normalize to clean data for resnet\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# load image, transform. Run through resnet using no_grad to speed up processing\n",
    "def extract_feature(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = transform(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        feat = model(x).squeeze().numpy()  \n",
    "    return feat\n",
    "\n",
    "features = []\n",
    "\n",
    "# get image data from row send through feature extraction, create list of processed data with filename, id, feature vector, and the labels\n",
    "for _, row in known_with_onehot.iterrows():\n",
    "    filename = row[\"filename\"]\n",
    "    img_path = os.path.join(\"data/img_align_celeba\", filename)\n",
    "\n",
    "    feature_vec = extract_feature(img_path)\n",
    "\n",
    "    features.append({\n",
    "        \"filename\": filename,\n",
    "        \"id\": row[\"id\"],\n",
    "        \"feature\": feature_vec,\n",
    "        **{col: row[col] for col in one_hot_labels.columns}\n",
    "    })\n",
    "\n",
    "feature_df = pd.DataFrame(features)\n",
    "# print(feature_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a239ee06-2356-49ef-82f1-557f86b82c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7444444444444445\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.82         9\n",
      "           1       0.62      0.89      0.73         9\n",
      "           2       0.83      0.56      0.67         9\n",
      "           3       0.62      0.89      0.73         9\n",
      "           4       0.71      0.56      0.62         9\n",
      "           5       0.89      0.89      0.89         9\n",
      "           6       0.73      0.89      0.80         9\n",
      "           7       0.80      0.89      0.84         9\n",
      "           8       0.78      0.78      0.78         9\n",
      "           9       0.75      0.33      0.46         9\n",
      "\n",
      "    accuracy                           0.74        90\n",
      "   macro avg       0.76      0.74      0.73        90\n",
      "weighted avg       0.76      0.74      0.73        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split processed data by id and features\n",
    "X = np.stack(feature_df[\"feature\"].values)\n",
    "\n",
    "y = feature_df[\"id\"].values\n",
    "\n",
    "# make y's 0-9 while maintaining relationship with filenames\n",
    "y_raw = feature_df[\"id\"].values\n",
    "unique_ids = np.unique(y_raw)\n",
    "id_to_idx = {old:i for i, old in enumerate(unique_ids)}\n",
    "y = np.array([id_to_idx[x] for x in y_raw])\n",
    "\n",
    "\n",
    "#train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# run data through linear SVC\n",
    "clf = SVC(kernel=\"linear\", probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb256e5-8a6c-46f8-986d-e8ed4b272011",
   "metadata": {},
   "source": [
    "The SVC classified 83% at 10% test size, 77% of the test samples at 20% test size and .64% at 30%. The aggressive fluctuations suggest the data set is too small to reliably learn the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e4e717a6-9abb-42a5-901f-3cf34ba85bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "probs = clf.predict_proba(X_test)\n",
    "# find percent of correct guesses within top 5 attempts\n",
    "def top_k_accuracy(probs, y_true, k=3):\n",
    "    k = min(k, probs.shape[1])\n",
    "    top_k = np.argsort(probs, axis=1)[:, -k:]\n",
    "    return np.mean([\n",
    "        y_true[i] in top_k[i]\n",
    "        for i in range(len(y_true))\n",
    "    ])\n",
    "\n",
    "print(\"Top-5 Accuracy:\", top_k_accuracy(probs, y_test, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e066d-fda3-4f2e-a8de-f6e796eebbea",
   "metadata": {},
   "source": [
    "We can find the correct id within 96% of attempts within the first five tries. Which isn't nearly as impressive as it sounds when there are only ten options. The top 2 attempts is correct over 84% however which is a strong foundation for a model. With more time and resources this model could effectively learn known faces with a high level of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "965e0565-50b2-4835-9fa2-7354beeef937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 1.7549\n",
      "Epoch 20/100, Loss: 1.3172\n",
      "Epoch 30/100, Loss: 1.0093\n",
      "Epoch 40/100, Loss: 0.7938\n",
      "Epoch 50/100, Loss: 0.6397\n",
      "Epoch 60/100, Loss: 0.5262\n",
      "Epoch 70/100, Loss: 0.4403\n",
      "Epoch 80/100, Loss: 0.3737\n",
      "Epoch 90/100, Loss: 0.3212\n",
      "Epoch 100/100, Loss: 0.2790\n",
      "Perceptron Accuracy: 0.7666666507720947\n"
     ]
    }
   ],
   "source": [
    "# same data prep as before except reformatted for torch\n",
    "y_raw = feature_df[\"id\"].values\n",
    "\n",
    "unique_ids = np.unique(y_raw)\n",
    "id_to_idx = {old:i for i, old in enumerate(unique_ids)}\n",
    "y = np.array([id_to_idx[x] for x in y_raw])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "input_dim = X_train.shape[1] \n",
    "\n",
    "class PerceptronClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# use cross entropy because it heavily punishes incorrect guesses\n",
    "model = PerceptronClassifier(input_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test).argmax(dim=1)\n",
    "    accuracy = (preds == y_test_t).float().mean().item()\n",
    "\n",
    "print(\"Perceptron Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3bdb678a-6777-404d-b4c4-a1de834f194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 0.0609\n",
      "Epoch 20/50, Loss: 0.0391\n",
      "Epoch 30/50, Loss: 0.0274\n",
      "Epoch 40/50, Loss: 0.0200\n",
      "Epoch 50/50, Loss: 0.0152\n",
      "Linear Regression Classifier Accuracy: 0.7333333492279053\n"
     ]
    }
   ],
   "source": [
    "# Create a one-hot encoded tensor for training labels\n",
    "y_train_onehot = torch.zeros(len(y_train), num_classes)\n",
    "y_train_onehot[torch.arange(len(y_train)), y_train] = 1.0\n",
    "\n",
    "\n",
    "# Create a one-hot encoded tensor for test labels\n",
    "y_test_onehot = torch.zeros(len(y_test), num_classes)\n",
    "y_test_onehot[torch.arange(len(y_test)), y_test] = 1.0\n",
    "\n",
    "\n",
    "# linear regression model\n",
    "class LinearRegressionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model_lr = LinearRegressionClassifier(input_dim, num_classes)\n",
    "\n",
    "# MSE used because it work well with one hot encoding\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# use torch's optimization model\n",
    "optimizer = torch.optim.Adam(model_lr.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    # Clear previous gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # input data to Linear regression model\n",
    "    outputs = model_lr(X_train)\n",
    "    loss = criterion(outputs, y_train_onehot)\n",
    "\n",
    "    # Backpropagate gradients and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# no_grad to save processing time\n",
    "with torch.no_grad():\n",
    "    # Get predicted class indices by taking argmax over class dimension\n",
    "    preds = model_lr(X_test).argmax(dim=1)\n",
    "    accuracy = (preds == y_test_t).float().mean().item()\n",
    "\n",
    "print(\"Linear Regression Classifier Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fff494-9daf-4b03-b247-bcfdc080bf47",
   "metadata": {},
   "source": [
    "The perceptron and Linear regression model both seem to perform pretty poorly when used for this dataset. They seem to overfit to the data really quickly suggesting there is not enough data for them to evaluate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83fd47-f6e4-466d-b6ea-724edc42c533",
   "metadata": {},
   "source": [
    "## Part 5: Conculsion\n",
    "\n",
    "Our goal of this project was to explore how different models and their achitecture affect preformance on a face recognition task using the CelebA dataset. We first trained our models on raw image data and then used a pretrained ResNet-50 combined with linear classifiers to evaluate the accuracy of those models. Despite their simplicity the next models in combination with the ResNet-50 achieved very good preformance which demonstrated the use of large-scale pretraining. \n",
    "\n",
    "Overall, this project reinforced the importance of selecting model architectures that align with the structure of the data. Given our problem/dataset was image based, models capable of preserving spacial information like the convolutional neural network or pretrained feature extractors were far better than baseline linear or full connected approches. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs345)",
   "language": "python",
   "name": "cs345"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
